#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ëª¨ë“ˆ
RAG ê²°ê³¼ + ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ì—¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
ë‚˜ì¤‘ì— ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
"""

import json
from typing import Dict, List, Any, Optional
from .transcript_summarizer import TranscriptSummarizer


class ContextBuilder:
    """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.summarizer = TranscriptSummarizer()
    
    def create_improved_system_prompt(self, transcript_summary: str, regulation: str, 
                                    summary: str, rag_context: str = "", analysis_result: Dict = None) -> str:
        """Qwenì— ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¶„ì„ ê²°ê³¼ í¬í•¨)"""
        
        # regulation JSONì„ ê°„ë‹¨íˆ ìš”ì•½
        if isinstance(regulation, str):
            try:
                reg_data = json.loads(regulation)
            except:
                reg_data = {}
        else:
            reg_data = regulation
        
        reg_summary = f"""ì¡¸ì—…í•™ì : {reg_data.get('graduation_credits', 130)}í•™ì 
ì „ê³µí•„ìˆ˜: {reg_data.get('major_required_credits', 45)}í•™ì 
ì „ê³µì„ íƒ: {reg_data.get('major_elective_credits', 30)}í•™ì """
        
        # ë¶„ì„ ê²°ê³¼ í¬í•¨
        analysis_section = ""
        if analysis_result and "error" not in analysis_result:
            # ê¸°ë³¸ ì •ë³´
            student_info = analysis_result.get("student_info", {})
            student_name = student_info.get('name', 'N/A')
            if isinstance(student_name, list):
                student_name = student_name[0] if student_name else 'N/A'
            
            basic_info = f"""ğŸ‘¤ ê¸°ë³¸ ì •ë³´:
- í•™ë²ˆ: {student_info.get('id', 'N/A')}
- ì´ë¦„: {student_name}
- ì „ê³µ: {student_info.get('major', 'N/A')}"""
            
            # ì¡¸ì—… ë¶„ì„ ê²°ê³¼ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ì¶¤)
            requirements_status = analysis_result.get("requirements_status", {})
            if requirements_status:
                total_credits = requirements_status.get("total_credits", {})
                major_credits = requirements_status.get("major_credits", {})
                capstone = requirements_status.get("capstone", {})
                english_cert = requirements_status.get("english_certification", {})
                gpa_info = requirements_status.get("gpa", {})
                
                # êµì–‘í•™ì  ê³„ì‚° (ì´í•™ì  - ì „ê³µí•™ì )
                general_credits_earned = total_credits.get("completed", 0) - major_credits.get("completed", 0)
                general_credits_required = total_credits.get("required", 130) - major_credits.get("required", 65)
                
                graduation_info = f"""ğŸ“ ì¡¸ì—… ìš”ê±´ ë¶„ì„ ê²°ê³¼:
- ì´ ì´ìˆ˜í•™ì : {total_credits.get('completed', 0)}í•™ì  / {total_credits.get('required', 130)}í•™ì  {'âœ…' if total_credits.get('satisfied', False) else 'âŒ'}
- ì „ê³µí•™ì : {major_credits.get('completed', 0)}í•™ì  / {major_credits.get('required', 65)}í•™ì  {'âœ…' if major_credits.get('satisfied', False) else 'âŒ'}
- êµì–‘í•™ì : {general_credits_earned}í•™ì  / {general_credits_required}í•™ì  {'âœ…' if general_credits_earned >= general_credits_required else 'âŒ'}
- ì¢…í•©ì„¤ê³„: {'ì´ìˆ˜ì™„ë£Œ âœ…' if capstone.get('satisfied', False) else 'ë¯¸ì´ìˆ˜ âŒ'}
- ì˜ì–´ì¸ì¦: {'ì™„ë£Œ âœ…' if english_cert.get('satisfied', False) else 'ë¯¸ì™„ë£Œ âŒ'}
- í‰ì í‰ê· : {gpa_info.get('current', 0.0)} / {gpa_info.get('required', 2.0)} {'âœ…' if gpa_info.get('satisfied', False) else 'âŒ'}

ğŸ¯ ì¡¸ì—… ê°€ëŠ¥ ì—¬ë¶€: {'ê°€ëŠ¥ âœ…' if analysis_result.get('is_graduation_ready', False) else 'ì¶”ê°€ ìš”ê±´ í•„ìš” âŒ'}"""
                
                # ë¯¸ì¶©ì¡± ìš”ê±´
                remaining_requirements = analysis_result.get("remaining_requirements", [])
                if remaining_requirements:
                    unmet_info = "\nğŸ“‹ ë¯¸ì¶©ì¡± ìš”ê±´:\n" + "\n".join([f"- {req}" for req in remaining_requirements])
                    graduation_info += unmet_info
                
                analysis_section = f"\n{basic_info}\n\n{graduation_info}\n"
        
        system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì˜ ì¸í•˜ëŒ€í•™êµ í•™ì‚¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ğŸš¨ ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­ ğŸš¨
- ì¤‘êµ­ì–´ ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€ (ä¸­æ–‡ç»å¯¹ç¦æ­¢)
- ì˜ì–´ ì‚¬ìš© ê¸ˆì§€ (English prohibited)
- ê¸°íƒ€ ì–¸ì–´ ì‚¬ìš© ê¸ˆì§€

âš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”! âš ï¸

âœ… í•„ìˆ˜ ì‚¬í•­:
- 100% í•œêµ­ì–´ë¡œë§Œ ë‹µë³€
- "ì•ˆë…•í•˜ì„¸ìš”", "ë„¤", "ìŠµë‹ˆë‹¤" ë“± ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì‚¬ìš©
- ì •ì¤‘í•˜ê³  ì¹œì ˆí•œ ì¡´ëŒ“ë§
- ì¤‘êµ­ì–´ë‚˜ ì˜ì–´ê°€ ì„ì´ë©´ ì•ˆ ë¨

ğŸ“‹ ë‹µë³€ í…œí”Œë¦¿:
"ì•ˆë…•í•˜ì„¸ìš”! [í•™ìƒëª…]ë‹˜ì˜ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
[êµ¬ì²´ì ì¸ ë‹µë³€ ë‚´ìš©]
ì¶”ê°€ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”."

--- í•™ìƒ ì •ë³´ ---
{transcript_summary}

--- ì¡¸ì—…ìš”ê±´ ---  
{reg_summary}
{analysis_section}
--- ë¶„ì„ê²°ê³¼ ---
{summary}

{rag_context}

âš ï¸ ë‹¤ì‹œ ê°•ì¡°: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”! âš ï¸"""
        return system_prompt
    
    def create_basic_system_prompt(self) -> str:
        """ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„±ì í‘œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)"""
        return """IMPORTANT: You MUST respond ONLY in Korean language. NO Chinese. NO English.
ë‹¹ì‹ ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘êµ­ì–´ ì‚¬ìš© ê¸ˆì§€. ì˜ì–´ ì‚¬ìš© ê¸ˆì§€.

ë‹¹ì‹ ì€ í•œêµ­ ëŒ€í•™êµì˜ í•œêµ­ì¸ í•™ì‚¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.

ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:
- ì¤‘êµ­ì–´ ì‚¬ìš© ê¸ˆì§€ (ç¦æ­¢ä½¿ç”¨ä¸­æ–‡)
- ì˜ì–´ ì‚¬ìš© ê¸ˆì§€ (No English allowed)

ì¤‘ìš” ê·œì¹™:
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
2. ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”  
3. í•™ìƒì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì¡°ì–¸ì„ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”

í•™ìƒì˜ ì§ˆë¬¸ì— ì¹œê·¼í•˜ê³  ì •í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    def create_rag_context(self, relevant_docs: List[str]) -> str:
        """RAG ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        if not relevant_docs:
            return ""
        
        semantic_context = "\n---\n".join(relevant_docs)
        return f"ê´€ë ¨ì •ë³´:\n{semantic_context}\n\n"
    
    def create_transcript_summary_context(self, transcript_data: Dict, 
                                        include_semester_details: bool = True,
                                        max_length: int = 3000) -> str:
        """ì„±ì í‘œ ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        # ê¸°ë³¸ ìš”ì•½ ì •ë³´
        basic_summary = self.summarizer.create_rag_document(transcript_data)
        
        if not include_semester_details:
            return basic_summary
        
        # í•™ê¸°ë³„ ìƒì„¸ ì •ë³´ ì¶”ê°€
        semester_details = self.create_semester_details(transcript_data)
        
        # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        full_context = f"{basic_summary}\n\n{semester_details}"
        
        # ê¸¸ì´ ì œí•œ ì ìš© (ë” ê´€ëŒ€í•˜ê²Œ)
        if len(full_context) > max_length:
            print(f"âš ï¸ ì„±ì í‘œ ì»¨í…ìŠ¤íŠ¸ê°€ {len(full_context)}ìë¡œ ê¸¸ì–´ì„œ {max_length}ìë¡œ ì¶•ì•½í•©ë‹ˆë‹¤.")
            # ê¸°ë³¸ ìš”ì•½ì€ ìœ ì§€í•˜ê³  í•™ê¸°ë³„ ì •ë³´ë§Œ ì¶•ì•½
            if len(basic_summary) < max_length * 0.4:
                remaining_length = max_length - len(basic_summary) - 100
                semester_details = semester_details[:remaining_length] + "\n[í•™ê¸°ë³„ ì •ë³´ê°€ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì¶•ì•½ë¨]"
                full_context = f"{basic_summary}\n\n{semester_details}"
            else:
                full_context = basic_summary[:max_length] + "\n[ê¸°ë³¸ ìš”ì•½ì´ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì¶•ì•½ë¨]"
        
        return full_context
    
    def create_semester_details(self, transcript_data: Dict) -> str:
        """í•™ê¸°ë³„ ìƒì„¸ ê³¼ëª© ì •ë³´ ìƒì„±"""
        semesters = transcript_data.get("ground_truth", {}).get("semesters", [])
        
        details = ["ğŸ“… í•™ê¸°ë³„ ìƒì„¸ ìˆ˜ê°• ê³¼ëª©:"]
        
        for semester in semesters:
            year_term = semester.get("year_term", "")
            subjects = semester.get("subjects", [])
            earned = semester.get("earned", 0)
            gpa = semester.get("gpa", 0.0)
            
            details.append(f"\nğŸ”¸ {year_term} ({earned}í•™ì , í‰ì  {gpa}):")
            
            # ê³¼ëª©ë³„ ì •ë³´
            for subject in subjects:
                name = subject.get("name", "")
                course_number = subject.get("course_number", "")
                credit = subject.get("credit", 0)
                grade = subject.get("grade", "")
                category = subject.get("category", "")
                
                details.append(f"  - {name} ({course_number}) [{category}] {credit}í•™ì , {grade}")
        
        return "\n".join(details)
    
    def create_basic_transcript_summary(self, transcript_data: Dict) -> str:
        """ê¸°ë³¸ ì„±ì í‘œ ìš”ì•½ë§Œ ìƒì„± (í•™ê¸°ë³„ ìƒì„¸ ì •ë³´ ì œì™¸)"""
        # í•™ìƒ ê¸°ë³¸ ì •ë³´ì™€ ì „ì²´ ìš”ì•½ë§Œ í¬í•¨
        basic_info = self.summarizer.extract_student_info(transcript_data)
        summary_info = self.summarizer.extract_summary_info(transcript_data)
        
        return f"{basic_info}\n\n{summary_info}"
    
    def create_full_context(self, user_question: str, transcript_data: Optional[Dict] = None,
                          rag_search_func=None, analysis_result: Optional[Dict] = None,
                          include_semester_details: bool = True) -> str:
        """ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (RAG + ì„±ì í‘œ ë¶„ì„ í†µí•©)"""
        
        if transcript_data is None:
            return self.create_basic_system_prompt()
        
        # RAG ê²€ìƒ‰ ìˆ˜í–‰ (ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰)
        rag_context = ""
        if rag_search_func:
            try:
                # ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥
                expanded_queries = self.expand_search_queries(user_question)
                all_docs = []
                
                for query in expanded_queries:
                    docs = rag_search_func(query, k=3, max_doc_length=1200)
                    all_docs.extend(docs)
                
                # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì„ ë³„
                unique_docs = self.deduplicate_and_rank_docs(all_docs, user_question)
                relevant_docs = unique_docs[:5]  # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
                
                rag_context = self.create_rag_context(relevant_docs)
                print(f"ğŸ” RAG ê²€ìƒ‰ ê²°ê³¼: {len(expanded_queries)}ê°œ ì¿¼ë¦¬ë¡œ {len(relevant_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œ ì œëª©ë“¤ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                for i, doc in enumerate(relevant_docs[:3], 1):
                    title = doc.split('\n')[0] if doc else "ì œëª© ì—†ìŒ"
                    print(f"  [{i}] {title[:50]}...")
                    
            except Exception as e:
                print(f"âš ï¸ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ì„±ì í‘œ ìš”ì•½ë§Œ ìƒì„± (RAGë¡œ í•„ìš”í•œ ì •ë³´ë§Œ ê°€ì ¸ì˜´)
        transcript_summary = self.create_basic_transcript_summary(transcript_data)
        
        # ë¶„ì„ ê²°ê³¼ ìš”ì•½
        if analysis_result:
            summary = analysis_result.get("summary", "ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
            regulation = analysis_result.get("regulation", {})
        else:
            summary = "ë¶„ì„ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            regulation = {}
        
        # ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        final_prompt = self.create_improved_system_prompt(
            transcript_summary=transcript_summary,
            regulation=regulation,
            summary=summary,
            rag_context=rag_context,
            analysis_result=analysis_result
        )
        
        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì •ë³´ ì¶œë ¥
        stats = self.get_context_statistics(final_prompt)
        print(f"ğŸ“Š í”„ë¡¬í”„íŠ¸ í†µê³„: {stats['character_count']}ì, {stats['word_count']}ë‹¨ì–´, ì˜ˆìƒí† í° {stats['estimated_tokens']}")
        
        return final_prompt
    
    def expand_search_queries(self, user_question: str) -> List[str]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥"""
        queries = [user_question]  # ì›ë³¸ ì§ˆë¬¸
        
        # í•™ê¸° ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬
        if "í•™ê¸°" in user_question:
            # ì—°ë„ì™€ í•™ê¸° ì¶”ì¶œ
            import re
            year_match = re.search(r'(\d{4})ë…„?ë„?', user_question)
            semester_match = re.search(r'(\d)í•™ê¸°', user_question)
            
            if year_match and semester_match:
                year = year_match.group(1)
                semester = semester_match.group(1)
                queries.extend([
                    f"{year}ë…„ë„ {semester}í•™ê¸°",
                    f"{year}ë…„ {semester}í•™ê¸°",
                    f"{year}-{semester}",
                ])
        
        # ê³¼ëª© ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬
        if any(keyword in user_question for keyword in ["ê³¼ëª©", "ìˆ˜ê°•", "ë“¤ì—ˆ", "ì´ìˆ˜"]):
            queries.extend([
                "ìˆ˜ê°• ì •ë³´",
                "ê³¼ëª© ì •ë³´",
                "í•™ê¸°ë³„ ìˆ˜ê°•"
            ])
        
        # ì„±ì  ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬
        if any(keyword in user_question for keyword in ["ì„±ì ", "í‰ì ", "í•™ì ", "A+", "F"]):
            queries.extend([
                "ì„±ì  íŒ¨í„´",
                "í‰ì  ì¶”ì´",
                "ì„±ì  ë¶„í¬"
            ])
        
        # ì¡¸ì—… ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬
        if any(keyword in user_question for keyword in ["ì¡¸ì—…", "ìš”ê±´", "ì¶©ì¡±"]):
            queries.extend([
                "ì¡¸ì—… ìš”ê±´",
                "ì¡¸ì—… ë¶„ì„",
                "ì˜ì–´ ì¸ì¦"
            ])
        
        return list(set(queries))  # ì¤‘ë³µ ì œê±°
    
    def deduplicate_and_rank_docs(self, docs: List[str], user_question: str) -> List[str]:
        """ë¬¸ì„œ ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ê¸°ë°˜ ìˆœìœ„ ë§¤ê¸°ê¸°"""
        if not docs:
            return []
        
        # ì¤‘ë³µ ì œê±° (ì²« ì¤„ ê¸°ì¤€)
        seen_titles = set()
        unique_docs = []
        
        for doc in docs:
            title = doc.split('\n')[0] if doc else ""
            if title not in seen_titles:
                seen_titles.add(title)
                unique_docs.append(doc)
        
        # ê´€ë ¨ì„± ê¸°ë°˜ ìˆœìœ„ ë§¤ê¸°ê¸° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        def calculate_relevance(doc: str) -> int:
            score = 0
            doc_lower = doc.lower()
            question_lower = user_question.lower()
            
            # ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­
            for word in question_lower.split():
                if len(word) > 1 and word in doc_lower:
                    score += 1
            
            # íŠ¹ë³„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
            if "í•™ê¸°" in question_lower and "í•™ê¸°" in doc_lower:
                score += 3
            if "ê³¼ëª©" in question_lower and any(keyword in doc_lower for keyword in ["ê³¼ëª©", "ìˆ˜ê°•"]):
                score += 2
            if "ì„±ì " in question_lower and any(keyword in doc_lower for keyword in ["ì„±ì ", "í‰ì "]):
                score += 2
                
            return score
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        ranked_docs = sorted(unique_docs, key=calculate_relevance, reverse=True)
        return ranked_docs
    
    def create_layered_context(self, user_question: str, context_layers: Dict[str, str]) -> str:
        """ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í–¥í›„ í™•ì¥ìš©)"""
        # TODO: ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë¡œì§ êµ¬í˜„
        # ì˜ˆ: í•™ìƒì •ë³´ ê³„ì¸µ, í•™ì‚¬ì •ë³´ ê³„ì¸µ, ì¡¸ì—…ìš”ê±´ ê³„ì¸µ ë“±
        pass
    
    def optimize_context_length(self, context: str, max_tokens: int = 28000) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”"""
        # ê°„ë‹¨í•œ ê¸¸ì´ ê¸°ë°˜ ìµœì í™” (í–¥í›„ í† í° ê¸°ë°˜ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥)
        estimated_tokens = len(context.split()) * 1.3  # ëŒ€ëµì ì¸ í† í° ì¶”ì •
        
        if estimated_tokens > max_tokens:
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ì•¼ í•˜ëŠ” ê²½ìš°
            target_length = int(len(context) * (max_tokens / estimated_tokens))
            context = context[:target_length] + "\n\n[ì»¨í…ìŠ¤íŠ¸ê°€ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì¸í•´ ì¶•ì•½ë˜ì—ˆìŠµë‹ˆë‹¤]"
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ {target_length} ë¬¸ìë¡œ ì¶•ì•½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return context
    
    def get_context_statistics(self, context: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ í†µê³„ ì •ë³´"""
        lines = context.split('\n')
        words = context.split()
        
        return {
            "character_count": len(context),
            "word_count": len(words),
            "line_count": len(lines),
            "estimated_tokens": int(len(words) * 1.3),
            "sections": len([line for line in lines if line.startswith("===")])
        }


class HierarchicalContextBuilder(ContextBuilder):
    """ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± í´ë˜ìŠ¤ (í–¥í›„ êµ¬í˜„)"""
    
    def __init__(self):
        super().__init__()
        self.context_layers = {
            "student": {"priority": 1, "max_tokens": 500},
            "academic": {"priority": 2, "max_tokens": 1000},
            "graduation": {"priority": 3, "max_tokens": 800},
            "rag": {"priority": 4, "max_tokens": 1200}
        }
    
    def create_layered_context(self, user_question: str, context_layers: Dict[str, str]) -> str:
        """ê³„ì¸µë³„ ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        # TODO: êµ¬ì²´ì ì¸ ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë¡œì§ êµ¬í˜„
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ í† í° í• ë‹¹ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        pass


# íŒ©í† ë¦¬ í•¨ìˆ˜
def create_context_builder() -> ContextBuilder:
    """ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ìƒì„±"""
    return ContextBuilder()


def create_hierarchical_context_builder() -> HierarchicalContextBuilder:
    """ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ìƒì„±"""
    return HierarchicalContextBuilder() 