#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëª¨ë¸ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
LLM ëª¨ë¸ ë¡œë”©ê³¼ ì¶”ë¡ ë§Œ ë‹´ë‹¹í•˜ë©° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±ì€ ì™¸ë¶€ì—ì„œ ë°›ìŒ
"""

import os
import torch
import time
import datetime
from typing import Dict, Any, Optional, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig


class ModelService:
    """LLM ëª¨ë¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str = "C:/Users/user/Desktop/DeepLearning/LLM/Qwen2.5-14B-Instruct"):
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None
        self.model_loaded = False
        
        # ë¡œê·¸ ê´€ë ¨
        self.log_dir = "conversation_logs"
        self._setup_log_directory()
        
        # CUDA ìµœì í™” ì„¤ì •
        self._setup_cuda_optimization()
    
    def _setup_cuda_optimization(self) -> None:
        """CUDA ìµœì í™” ì„¤ì •"""
        print("âš¡ PyTorch ë° CUDA ìµœì í™” ì„¤ì • ì¤‘...")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:256"
        os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # ë””ë²„ê¹…ìš©
        
        if torch.cuda.is_available():
            # CUDA ìµœì í™” ì„¤ì •
            torch.backends.cudnn.enabled = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            print("âœ… CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
            print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ“Š CUDA ë²„ì „: {torch.version.cuda}")
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    def _setup_log_directory(self) -> None:
        """ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
            print(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±: {self.log_dir}")
    
    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë”©"""
        try:
            print("ğŸš€ Qwen 2.5 14B ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... (24GB VRAMì— ìµœì í™”)")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # 4bit ì–‘ìí™” ì„¤ì •
            print("ğŸ“¦ Qwen 2.5 14B ëª¨ë¸ ë¡œë”© ì¤‘... (4bit ì–‘ìí™”ë¡œ ì†ë„ í–¥ìƒ)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                quantization_config=quantization_config,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                max_memory={0: "20GB", "cpu": "16GB"},
            )
            
            # ëª¨ë¸ ë¡œë”© í›„ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            if self.device == "cuda":
                print(f"âœ… ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
                print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f}GB")
                torch.cuda.empty_cache()
                print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
            self.model_loaded = True
            print("âœ… Qwen 2.5 14B ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (4bit ì–‘ìí™”)")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.model_loaded = False
            return False
    
    def generate_response(self, messages: list, max_new_tokens: int = 512, 
                         do_sample: bool = False, **kwargs) -> Tuple[str, Dict[str, Any]]:
        """ì‘ë‹µ ìƒì„±"""
        if not self.model_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        start_time = time.time()
        initial_gpu_memory = torch.cuda.memory_allocated() / 1024**3 if self.device == "cuda" else 0
        
        # ë©”ì‹œì§€ë¥¼ ì±„íŒ… í…œí”Œë¦¿ì— ì ìš©
        chat_template = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer.encode_plus(
            chat_template,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=28000
        )
        
        # GPUë¡œ ì…ë ¥ ë°ì´í„° ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # CUDA ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if self.device == "cuda":
            print(f"ğŸ’¾ ì¶”ë¡  ì „ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
        
        # ì¶”ë¡  ìˆ˜í–‰
        print("ğŸ¤– Qwen 2.5 14B ì¶”ë¡  ì¤‘...")
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True,
                # ê²½ê³  ë°©ì§€ë¥¼ ìœ„í•´ sampling ê´€ë ¨ íŒŒë¼ë¯¸í„° ëª…ì‹œì ìœ¼ë¡œ None ì„¤ì •
                temperature=None,
                top_p=None,
                top_k=None,
                **kwargs
            )
        
        # ì‘ë‹µ ì¶”ì¶œ
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        input_text = self.tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
        
        if full_response.startswith(input_text):
            response = full_response[len(input_text):].strip()
        else:
            if "<|im_start|>assistant" in full_response:
                response = full_response.split("<|im_start|>assistant")[-1].strip()
                if response.startswith("\n"):
                    response = response[1:]
            else:
                response = full_response
        
        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        if response.startswith("\n"):
            response = response[1:]
        if "<|im_end|>" in response:
            response = response.replace("<|im_end|>", "").strip()
        
        print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
        
        # ì²˜ë¦¬ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘
        end_time = time.time()
        processing_time = end_time - start_time
        final_gpu_memory = torch.cuda.memory_allocated() / 1024**3 if self.device == "cuda" else 0
        
        metadata = {
            "processing_time_seconds": round(processing_time, 2),
            "input_tokens": len(self.tokenizer.encode(chat_template)),
            "output_tokens": len(self.tokenizer.encode(response)),
            "gpu_memory_initial_gb": round(initial_gpu_memory, 2),
            "gpu_memory_final_gb": round(final_gpu_memory, 2),
            "model_name": self.model_path,
            "quantization": "4bit",
            "device": self.device
        }
        
        # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == "cuda":
            print(f"ğŸ’¾ ì¶”ë¡  í›„ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        return response, metadata
    
    def save_conversation_log(self, user_question: str, model_response: str, 
                            metadata: Dict[str, Any], system_prompt: str) -> None:
        """ëŒ€í™” ë¡œê·¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ TXT íŒŒì¼ì— ì €ì¥"""
        today = datetime.datetime.now().strftime("%Y%m%d")
        log_file = os.path.join(self.log_dir, f"qwen_conversation_{today}.txt")
        
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        memory_usage = f"{torch.cuda.memory_allocated() / 1024**3:.2f}GB" if torch.cuda.is_available() else "N/A"
        
        log_entry = f"""
{'='*80}
ëŒ€í™” ê¸°ë¡ #{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}
{'='*80}
â° ì‹œê°„: {timestamp}
ğŸ–¥ï¸ ì¥ì¹˜: {metadata.get('device', 'N/A')}
ğŸ’¾ GPU ë©”ëª¨ë¦¬: {memory_usage}
âš¡ ì²˜ë¦¬ ì‹œê°„: {metadata.get('processing_time_seconds', 0):.2f}ì´ˆ

ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:
{system_prompt}

â“ ì‚¬ìš©ì ì§ˆë¬¸:
{user_question}

ğŸ¤– AI ì‘ë‹µ:
{model_response}

ğŸ“Š í†µê³„:
- ì§ˆë¬¸ ê¸¸ì´: {len(user_question)}ì
- ì‘ë‹µ ê¸¸ì´: {len(model_response)}ì
- ì…ë ¥ í† í°: {metadata.get('input_tokens', 'N/A')}ê°œ
- ì¶œë ¥ í† í°: {metadata.get('output_tokens', 'N/A')}ê°œ

"""
        
        try:
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(log_entry)
            print(f"ğŸ’¾ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_file}")
        except Exception as e:
            print(f"âŒ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_path": self.model_path,
            "device": self.device,
            "model_loaded": self.model_loaded,
            "gpu_available": torch.cuda.is_available(),
            "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0,
            "gpu_memory_allocated": torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
        }
    
    def cleanup(self) -> None:
        """ëª¨ë¸ ë° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.model:
            del self.model
            self.model = None
        
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.model_loaded = False
        print("ğŸ—‘ï¸ ëª¨ë¸ ë° ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


# íŒ©í† ë¦¬ í•¨ìˆ˜
def create_model_service(model_path: str = "C:/Users/user/Desktop/DeepLearning/LLM/Qwen2.5-14B-Instruct") -> ModelService:
    """ëª¨ë¸ ì„œë¹„ìŠ¤ ìƒì„±"""
    return ModelService(model_path) 