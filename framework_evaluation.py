#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê¸°ì¡´ evaluation.pyë¥¼ í™œìš©í•œ í”„ë ˆì„ì›Œí¬ ë¹„êµ
"""

import time
from datetime import datetime

# LangChain
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms.base import LLM
from langchain.schema import Document

# LlamaIndex  
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.llms import CustomLLM, CompletionResponse, LLMMetadata
from llama_index.core.schema import Document as LlamaDocument
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# ê¸°ì¡´ evaluation ëª¨ë“ˆ
from evaluation import ClaudeEvaluationExporter


class QwenLangChainLLM(LLM):
    """Qwenì„ LangChain LLMìœ¼ë¡œ ë˜í•‘"""
    def __init__(self, model_service):
        super().__init__()
        self.model_service = model_service
    
    def _call(self, prompt: str, stop=None) -> str:
        messages = [{"role": "user", "content": prompt}]
        response, _ = self.model_service.generate_response(messages)
        return response
    
    @property
    def _llm_type(self) -> str:
        return "qwen"


class QwenLlamaIndexLLM(CustomLLM):
    """Qwenì„ LlamaIndex LLMìœ¼ë¡œ ë˜í•‘"""
    def __init__(self, model_service):
        super().__init__()
        self.model_service = model_service
    
    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(context_window=32768, num_output=512, model_name="qwen2.5-14b")
    
    def complete(self, prompt: str, **kwargs) -> CompletionResponse:
        messages = [{"role": "user", "content": prompt}]
        response, _ = self.model_service.generate_response(messages)
        return CompletionResponse(text=response)


class LangChainService:
    """LangChain ê¸°ë°˜ ì„œë¹„ìŠ¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ ë™ì¼)"""
    def __init__(self, model_service, document_manager):
        self.model_service = model_service
        self.qa_chain = self._setup_langchain(model_service, document_manager)
        
    def _setup_langchain(self, model_service, document_manager):
        # ì„ë² ë”© ì„¤ì •
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        # ë¬¸ì„œ ë¡œë“œ
        rag_data = document_manager.load_rag_dataset()
        documents = [Document(page_content=item.get('content', '')) for item in rag_data]
        
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = Chroma.from_documents(documents, embeddings)
        
        # LLM ë° ì²´ì¸ ìƒì„±
        llm = QwenLangChainLLM(model_service)
        return RetrievalQA.from_chain_type(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 5})
        )
    
    def ask_question(self, question):
        """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ ë™ì¼"""
        try:
            return self.qa_chain.run(question)
        except Exception as e:
            return f"ERROR: {str(e)}"
    
    def get_system_status(self):
        return {"model_loaded": True, "framework": "LangChain"}
    
    def cleanup(self):
        pass


class LlamaIndexService:
    """LlamaIndex ê¸°ë°˜ ì„œë¹„ìŠ¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ ë™ì¼)"""
    def __init__(self, model_service, document_manager):
        self.model_service = model_service
        self.query_engine = self._setup_llamaindex(model_service, document_manager)
        
    def _setup_llamaindex(self, model_service, document_manager):
        # ì„ë² ë”© ë° LLM ì„¤ì •
        embed_model = HuggingFaceEmbedding(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        llm = QwenLlamaIndexLLM(model_service)
        
        Settings.llm = llm
        Settings.embed_model = embed_model
        
        # ë¬¸ì„œ ë¡œë“œ
        rag_data = document_manager.load_rag_dataset()
        documents = [LlamaDocument(text=item.get('content', '')) for item in rag_data]
        
        # ì¸ë±ìŠ¤ ìƒì„±
        index = VectorStoreIndex.from_documents(documents)
        return index.as_query_engine(similarity_top_k=5)
    
    def ask_question(self, question):
        """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ ë™ì¼"""
        try:
            response = self.query_engine.query(f"í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: {question}")
            return str(response)
        except Exception as e:
            return f"ERROR: {str(e)}"
    
    def get_system_status(self):
        return {"model_loaded": True, "framework": "LlamaIndex"}
    
    def cleanup(self):
        pass


def run_framework_comparison():
    """ì„¸ í”„ë ˆì„ì›Œí¬ ë¹„êµ ì‹¤í–‰"""
    print("ğŸ¯ RAG í”„ë ˆì„ì›Œí¬ ë¹„êµ í‰ê°€ ì‹œì‘")
    
    # ê¸°ì¡´ ì„œë¹„ìŠ¤ ë¡œë“œ
    from modules import create_academic_service
    from modules.document_manager import DocumentManager
    
    original_service = create_academic_service()
    document_manager = DocumentManager()
    
    results = {}
    
    # 1. Custom RAG í‰ê°€
    print("\nğŸ¯ Custom RAG í‰ê°€...")
    evaluator = ClaudeEvaluationExporter()
    evaluator.output_file = f"custom_rag_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    results['custom'] = evaluator.run_comprehensive_evaluation(original_service, 4)
    
    # 2. LangChain RAG í‰ê°€
    print("\nğŸ¦œ LangChain RAG í‰ê°€...")
    langchain_service = LangChainService(original_service.model_service, document_manager)
    evaluator.output_file = f"langchain_rag_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    results['langchain'] = evaluator.run_comprehensive_evaluation(langchain_service, 4)
    
    # 3. LlamaIndex RAG í‰ê°€
    print("\nğŸ¦™ LlamaIndex RAG í‰ê°€...")
    llamaindex_service = LlamaIndexService(original_service.model_service, document_manager)
    evaluator.output_file = f"llamaindex_rag_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    results['llamaindex'] = evaluator.run_comprehensive_evaluation(llamaindex_service, 4)
    
    # ì •ë¦¬
    original_service.cleanup()
    
    # ìš”ì•½ ìƒì„±
    summary_file = f"framework_comparison_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"""
RAG í”„ë ˆì„ì›Œí¬ ì•„í‚¤í…ì²˜ ë¹„êµ í‰ê°€ ì™„ë£Œ
=======================================

ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

ìƒì„±ëœ íŒŒì¼:
- Custom RAG: {results['custom']}
- LangChain RAG: {results['langchain']}  
- LlamaIndex RAG: {results['llamaindex']}

ê° íŒŒì¼ì„ Claudeì—ê²Œ ì—…ë¡œë“œí•˜ì—¬ RAGAS í‰ê°€ë¥¼ ìš”ì²­í•˜ì„¸ìš”.
""")
    
    print(f"\nğŸ‰ ë¹„êµ í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š ìš”ì•½ íŒŒì¼: {summary_file}")
    for framework, file in results.items():
        print(f"ğŸ“ {framework}: {file}")
    
    return results


if __name__ == "__main__":
    run_framework_comparison()
